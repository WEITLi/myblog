---
title: "Bayes_work"
author: ''
date: '2022-06-30'
slug: bayes-work
categories: []
tags: []
description: ''
featured_image: ''
images: []
comment: yes
---

<script src="{{< blogdown/postref >}}index_files/htmlwidgets/htmlwidgets.js"></script>
<script src="{{< blogdown/postref >}}index_files/d3/d3.min.js"></script>
<script src="{{< blogdown/postref >}}index_files/forceNetwork-binding/forceNetwork.js"></script>


<pre class="r"><code>library(stringr)
library(sentimentr)
library(dplyr, quietly = TRUE)
library(reshape2)
library(tidyr)
library(tidytext)
setwd(&#39;D:/R-4.0.0/RStudio.workfile&#39;)
tweet.df &lt;- read.csv(&quot;en_china_threat.csv&quot;,header = T,encoding = &#39;UTF-8&#39;)
tweet.df &lt;- tweet.df[1:1000,]
colnames(tweet.df) &lt;- c(&quot;id&quot;,&quot;text&quot;)
tweet.df &lt;- data.frame(tweet.df[&quot;text&quot;])
#查看每个句子大概包含多少个字符
charnum &lt;- as.vector(sapply(tweet.df,nchar))
hist(charnum,breaks = 20)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>library(tm)
## 初步数据清洗
tweet.df$text =str_replace_all(tweet.df$text,&quot;[\\.\\,\\;]+&quot;, &quot; &quot;)
tweet.df$text =str_replace_all(tweet.df$text,&quot;http\\w+&quot;, &quot;&quot;)
tweet.df$text =str_replace_all(tweet.df$text,&quot;@\\w+&quot;, &quot; &quot;)
tweet.df$text =str_replace_all(tweet.df$text,&quot;[[:punct:]]&quot;, &quot; &quot;)
tweet.df$text =str_replace_all(tweet.df$text,&quot;[[:digit:]]&quot;, &quot; &quot;)
tweet.df$text =str_replace_all(tweet.df$text,&quot;^ &quot;, &quot; &quot;)
tweet.df$text =str_replace_all(tweet.df$text,&quot;[&lt;].*[&gt;]&quot;, &quot; &quot;) 

library(caret)
#使用sentiment()函数得到情感评分
sentiment.score &lt;- sentiment(tweet.df$text)
tweet.df$polarity &lt;- sentiment.score$sentiment
tweet.final &lt;- tweet.df[,c(&#39;text&#39;,&#39;polarity&#39;)]
tweet.final$sentiment &lt;- ifelse(tweet.final$polarity &lt;0, &quot;Negative&quot;,&quot;Positive&quot;)
table(tweet.final$sentiment)</code></pre>
<pre><code>## 
## Negative Positive 
##      700      300</code></pre>
<pre class="r"><code>tweet.final$sentiment &lt;- as.factor(tweet.final$sentiment)
#平衡数据集,重复在positive中采样
tweet.final &lt;- upSample(x = tweet.final$text, y = tweet.final$sentiment)
names(tweet.final) &lt;- c(&#39;text&#39;, &#39;sentiment&#39;)
head(tweet.final)</code></pre>
<pre><code>##                                                                                                                                                                                                                                                                                                                                                  text
## 1                                                                                                                                             Korea We Willposted by Rob Quinn  Newser Staff nbspApr       nbspPresident Trump is taking a tough line on the North Korean nuclear threat days ahead of his first meeting with his Chinese counterpart
## 2 Global tradeTrade wars wounded Companies improvise to dodge cost hikesBy PAUL WISEMANJanuary        WASHINGTON AP  In Rochester New York a maker of furnaces for semiconductor and solar companies is moving its research and development to China to dodge President Donald Trumps import taxes  a move that threatens a handful of its    US jobs
## 3                                                                                                                                                                                                                                                                                          They said if I left China it would threaten state security
## 4                                                                                                                                                                                    Its in our interest to end the uncertainty of    percent tariffs on imports from China a threat which hangs like a Sword of Damocles over American manufacturers
## 5                                                                                                                                                                                                                           Trump has ultimately threatened tariffs on over     billion in Chinese goods covering virtually all US imports from China
## 6                                                                                                                                                                                                  The administration has imposed billions of dollars in tariffs on Chinese and Canadian products and threatened billions more on US allies in Europe
##   sentiment
## 1  Negative
## 2  Negative
## 3  Negative
## 4  Negative
## 5  Negative
## 6  Negative</code></pre>
<pre class="r"><code>table(tweet.final$sentiment)</code></pre>
<pre><code>## 
## Negative Positive 
##      700      700</code></pre>
<pre class="r"><code>tweet.final$id &lt;- seq(1, nrow(tweet.final))

tweet.text &lt;- tweet.final[,1]
#创建语料库
tweet.text &lt;- Corpus(VectorSource(tweet.text))
#去除停用词
tweet.text &lt;- tm_map(tweet.text,removeWords,stopwords())

tweet.dtm &lt;- DocumentTermMatrix(tweet.text)

freq.terms &lt;- sort(colSums(as.matrix(tweet.dtm)),decreasing = TRUE)
freq.terms &lt;- data.frame(name = names(freq.terms),
                         fre = freq.terms,row.names = NULL)
head(freq.terms,10)</code></pre>
<pre><code>##          name fre
## 1       china 941
## 2     chinese 652
## 3      threat 591
## 4       trump 329
## 5     tariffs 309
## 6       trade 291
## 7     threats 263
## 8  threatened 241
## 9     billion 189
## 10        the 186</code></pre>
<pre class="r"><code>length(which(freq.terms$fre&gt;100))</code></pre>
<pre><code>## [1] 27</code></pre>
<pre class="r"><code>library(wordcloud)
set.seed(623) # to make it reproducible


## 将文本数据从语料库中转化为dataframe
polaritydf &lt;- data.frame(text=sapply(tweet.text, identity), stringsAsFactors=F)
polaritydf$label &lt;- rep(c(&quot;pos&quot;,&quot;neg&quot;),each = length(polaritydf$text)/2)
## 对比分析两种情感评论的用词长度
wordlist &lt;- str_split(polaritydf$text,&quot;[[:space:]]+&quot;)
wordlen &lt;- unlist(lapply(wordlist,length))
polaritydf$wordlen &lt;- wordlen


##negtive word 的长度稍长
wordfre &lt;- polaritydf%&gt;%unnest_tokens(word,text)%&gt;%
  group_by(label,word)%&gt;%
  summarise(Fre = n())%&gt;%
  arrange(desc(Fre)) %&gt;%
  acast(word~label,value.var = &quot;Fre&quot;,fill = 0)</code></pre>
<pre class="r"><code>## 可视化两种类型情感对比的词云
comparison.cloud(wordfre,scale=c(4,.5),max.words=180,
                 title.size=1.5,colors = c(&quot;darkgreen&quot;,&quot;darkred&quot;))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>#找出出现较为频繁的词语,出现频率大于5
dict &lt;- names(which(wordfre[,1]+wordfre[,2] &gt;5))
tw.dtm &lt;- DocumentTermMatrix(tweet.text,control = list(dictionary = dict))
#由于矩阵比较 稀疏,再控制一下稀疏程度
tw.dtm &lt;- removeSparseTerms(tw.dtm,0.99)</code></pre>
<pre class="r"><code>library(lda)
library(LDAvis)
library(servr)
term_table &lt;- table(unlist(wordlist))
term_table &lt;- sort(term_table, decreasing = TRUE)
term_table[1:20]</code></pre>
<pre><code>## 
##      China               Chinese     threat         US      Trump    tariffs 
##        938        736        652        587        529        328        302 
##    threats threatened      trade    billion        The          s       said 
##        261        241        238        188        180        173        170 
##  President      North      goods   security      Korea     Russia 
##        164        161        154        136        129        129</code></pre>
<pre class="r"><code>dim(term_table)</code></pre>
<pre><code>## [1] 7210</code></pre>
<pre class="r"><code>## 剔除空的数据,否则json报错
del &lt;- (names(term_table)) %in% c(&quot;&quot;)
term_table &lt;- term_table[!del]
dim(term_table)</code></pre>
<pre><code>## [1] 7209</code></pre>
<pre class="r"><code># 删除出现次数小于10次的词语
del &lt;- term_table &lt; 10
term_table &lt;- term_table[!del]
vocab &lt;- names(term_table)

# 将数据整理为lda包可使用的形式
get_terms &lt;- function(x) {
  index &lt;- match(x, vocab)
  index &lt;- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents &lt;- lapply(wordlist, get_terms)

## documents[[i]]：表示第i个文档
## documents[[i]][1,j] : 表示文档i中第j个单词对应在词库中的（索引－1）
## documents[[i]][2,j] : 表示文档i中第j个单词出现的次数

## 使用lda模型
alpha &lt;-  0.02;eta &lt;- 0.02

t1 &lt;- Sys.time()
fit &lt;- lda.collapsed.gibbs.sampler(documents = documents, K = 3, 
                                   vocab = vocab, 
                                   num.iterations = 50, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 &lt;- Sys.time()
t2 - t1  # about 30 sec on laptop</code></pre>
<pre><code>## Time difference of 0.1770411 secs</code></pre>
<pre class="r"><code>dim(fit$topics)</code></pre>
<pre><code>## [1]   3 571</code></pre>
<pre class="r"><code>## 可视化lda模型
## 对模型进行可视化

theta &lt;- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi &lt;- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
doc_length &lt;- sapply(documents, function(x) sum(x[2, ]))
term_frequency &lt;- as.vector(term_table)
pol_ldavis &lt;- list(phi = phi,theta = theta,doc_length = doc_length,
                   vocab = vocab,term_frequency = term_frequency)

# create the JSON object to feed the visualization:
json &lt;- createJSON(phi = pol_ldavis$phi, 
                   theta = pol_ldavis$theta, 
                   doc.length = pol_ldavis$doc_length, 
                   vocab = pol_ldavis$vocab, 
                   term.frequency = pol_ldavis$term_frequency)

serVis(json, out.dir = &quot;vis&quot;)</code></pre>
<div id="networdk3d" class="section level2">
<h2>Networdk3D</h2>
<pre class="r"><code>library(networkD3)
library(igraph)
library(RTextTools)
library(magrittr)
#bigram
TweetDf &lt;- data.frame(as.character(tweet.final$text))
names(TweetDf) &lt;- &quot;text&quot;
bi.gram.words &lt;- TweetDf %&gt;% 
  unnest_tokens(
    input = text, 
    output = bigram, 
    token = &#39;ngrams&#39;, 
    n = 2
  ) %&gt;% 
  filter(! is.na(bigram))
extra.stop.words &lt;- c(&#39;https&#39;)
stopwords.df &lt;- tibble(
  word = c(stopwords(kind = &#39;es&#39;),
           stopwords(kind = &#39;en&#39;),
           extra.stop.words)
)
bi.gram.words %&lt;&gt;% 
  separate(col = bigram, into = c(&#39;word1&#39;, &#39;word2&#39;), sep = &#39; &#39;) %&gt;% 
  filter(! word1 %in% stopwords.df$word) %&gt;% 
  filter(! word2 %in% stopwords.df$word) %&gt;% 
  filter(! is.na(word1)) %&gt;% 
  filter(! is.na(word2)) 
bi.gram.count &lt;- bi.gram.words %&gt;% 
  dplyr::count(word1, word2, sort = TRUE) %&gt;% 
  dplyr::rename(weight = n)


threshold &lt;- 20
filter=dplyr::filter
network &lt;-  bi.gram.count %&gt;%
  filter(weight &gt; threshold) %&gt;%
  graph_from_data_frame(directed = FALSE)

# Store the degree.
V(network)$degree &lt;- strength(graph = network)
# Compute the weight shares.
E(network)$width &lt;- E(network)$weight/max(E(network)$weight)

# Create networkD3 object.
network.D3 &lt;- igraph_to_networkD3(g = network)
# Define node size.
network.D3$nodes %&lt;&gt;% mutate(Degree = (1E-2)*V(network)$degree)
# Define color group
network.D3$nodes %&lt;&gt;% mutate(Group = 1)
# Define edges width. 
network.D3$links$Width &lt;- 10*E(network)$width

forceNetwork(
  Links = network.D3$links, 
  Nodes = network.D3$nodes, 
  Source = &#39;source&#39;, 
  Target = &#39;target&#39;,
  NodeID = &#39;name&#39;,
  Group = &#39;Group&#39;, 
  opacity = 0.9,
  Value = &#39;Width&#39;,
  Nodesize = &#39;Degree&#39;, 
  # We input a JavaScript function.
  linkWidth = JS(&quot;function(d) { return Math.sqrt(d.value); }&quot;), 
  fontSize = 12,
  zoom = TRUE, 
  opacityNoHover = 1
)</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="forceNetwork html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"links":{"source":[12,6,5,2,16,2,15,5,0,0,0,3,9,12,8,1,11,10,14,12,13,3,4,6,7,4],"target":[25,32,9,19,31,22,27,17,17,28,29,4,23,23,14,18,24,24,26,30,33,12,13,20,21,15],"value":[10,9.89473684210526,9.78947368421053,9.26315789473684,9.26315789473684,8,7.26315789473684,5.57894736842105,5.26315789473684,4.84210526315789,4.52631578947368,4,3.36842105263158,3.36842105263158,3.26315789473684,3.15789473684211,2.84210526315789,2.84210526315789,2.73684210526316,2.63157894736842,2.63157894736842,2.52631578947368,2.42105263157895,2.31578947368421,2.21052631578947,2.21052631578947],"colour":["#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666"]},"nodes":{"name":["north","united","chinese","donald","president","south","trade","billion","national","china","new","impose","trump","us","security","xi","white","korea","states","goods","war","worth","imports","s","tariffs","administration","threat","jinping","korean","koreas","threatened","house","agreement","treasury"],"group":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"nodesize":[1.45,0.94,1.43,1.76,1.45,1.06,0.9,0.53,0.46,1.19,0.38,0.32,1.67,0.52,0.73,0.53,0.22,1.25,0.94,0.93,0.69,0.53,0.5,0.67,0.7,0.32,0.27,0.27,0.25,0.25,0.23,0.22,0.21,0.21]},"options":{"NodeID":"name","Group":"Group","colourScale":"d3.scaleOrdinal(d3.schemeCategory20);","fontSize":12,"fontFamily":"serif","clickTextSize":30,"linkDistance":50,"linkWidth":"function(d) { return Math.sqrt(d.value); }","charge":-30,"opacity":0.9,"zoom":true,"legend":false,"arrows":false,"nodesize":true,"radiusCalculation":" Math.sqrt(d.nodesize)+6","bounded":false,"opacityNoHover":1,"clickAction":null}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="lda可视化动图" class="section level2">
<h2>LDA可视化动图</h2>
<p><img src="images/gif_LDA.gif" /></p>
</div>
