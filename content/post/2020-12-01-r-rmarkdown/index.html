---
title: "Hello R Markdown"
author: "Frida Gomam"
date: '2020-12-01T21:13:14-05:00'
categories: R
tags:
- R Markdown
- plot
- regression
---

<script src="{{< blogdown/postref >}}index_files/htmlwidgets/htmlwidgets.js"></script>
<script src="{{< blogdown/postref >}}index_files/d3/d3.min.js"></script>
<script src="{{< blogdown/postref >}}index_files/forceNetwork-binding/forceNetwork.js"></script>


<pre class="r"><code>library(stringr)
## Warning: 程辑包&#39;stringr&#39;是用R版本4.0.5 来建造的
library(sentimentr)
## Warning: 程辑包&#39;sentimentr&#39;是用R版本4.0.5 来建造的
library(dplyr, quietly = TRUE)
## 
## 载入程辑包：&#39;dplyr&#39;
## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag
## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union
library(reshape2)
## Warning: 程辑包&#39;reshape2&#39;是用R版本4.0.5 来建造的
library(tidyr)
## Warning: 程辑包&#39;tidyr&#39;是用R版本4.0.5 来建造的
## 
## 载入程辑包：&#39;tidyr&#39;
## The following object is masked from &#39;package:reshape2&#39;:
## 
##     smiths
library(tidytext)
setwd(&#39;D:/R-4.0.0/RStudio.workfile&#39;)
tweet.df &lt;- read.csv(&quot;en_china_threat.csv&quot;,header = T,encoding = &#39;UTF-8&#39;)
tweet.df &lt;- tweet.df[1:1000,]
colnames(tweet.df) &lt;- c(&quot;id&quot;,&quot;text&quot;)
tweet.df &lt;- data.frame(tweet.df[&quot;text&quot;])
#查看每个句子大概包含多少个字符
charnum &lt;- as.vector(sapply(tweet.df,nchar))
hist(charnum,breaks = 20)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>library(tm)
## Warning: 程辑包&#39;tm&#39;是用R版本4.0.5 来建造的
## 载入需要的程辑包：NLP
## Warning: 程辑包&#39;NLP&#39;是用R版本4.0.3 来建造的
## 初步数据清洗
tweet.df$text =str_replace_all(tweet.df$text,&quot;[\\.\\,\\;]+&quot;, &quot; &quot;)
tweet.df$text =str_replace_all(tweet.df$text,&quot;http\\w+&quot;, &quot;&quot;)
tweet.df$text =str_replace_all(tweet.df$text,&quot;@\\w+&quot;, &quot; &quot;)
tweet.df$text =str_replace_all(tweet.df$text,&quot;[[:punct:]]&quot;, &quot; &quot;)
tweet.df$text =str_replace_all(tweet.df$text,&quot;[[:digit:]]&quot;, &quot; &quot;)
tweet.df$text =str_replace_all(tweet.df$text,&quot;^ &quot;, &quot; &quot;)
tweet.df$text =str_replace_all(tweet.df$text,&quot;[&lt;].*[&gt;]&quot;, &quot; &quot;) 

library(caret)
## Warning: 程辑包&#39;caret&#39;是用R版本4.0.5 来建造的
## 载入需要的程辑包：ggplot2
## 
## 载入程辑包：&#39;ggplot2&#39;
## The following object is masked from &#39;package:NLP&#39;:
## 
##     annotate
## 载入需要的程辑包：lattice
#使用sentiment()函数得到情感评分
sentiment.score &lt;- sentiment(tweet.df$text)
tweet.df$polarity &lt;- sentiment.score$sentiment
tweet.final &lt;- tweet.df[,c(&#39;text&#39;,&#39;polarity&#39;)]
tweet.final$sentiment &lt;- ifelse(tweet.final$polarity &lt;0, &quot;Negative&quot;,&quot;Positive&quot;)
table(tweet.final$sentiment)
## 
## Negative Positive 
##      700      300

tweet.final$sentiment &lt;- as.factor(tweet.final$sentiment)
#平衡数据集,重复在positive中采样
tweet.final &lt;- upSample(x = tweet.final$text, y = tweet.final$sentiment)
names(tweet.final) &lt;- c(&#39;text&#39;, &#39;sentiment&#39;)
head(tweet.final)
##                                                                                                                                                                                                                                                                                                                                                  text
## 1                                                                                                                                             Korea We Willposted by Rob Quinn  Newser Staff nbspApr       nbspPresident Trump is taking a tough line on the North Korean nuclear threat days ahead of his first meeting with his Chinese counterpart
## 2 Global tradeTrade wars wounded Companies improvise to dodge cost hikesBy PAUL WISEMANJanuary        WASHINGTON AP  In Rochester New York a maker of furnaces for semiconductor and solar companies is moving its research and development to China to dodge President Donald Trumps import taxes  a move that threatens a handful of its    US jobs
## 3                                                                                                                                                                                                                                                                                          They said if I left China it would threaten state security
## 4                                                                                                                                                                                    Its in our interest to end the uncertainty of    percent tariffs on imports from China a threat which hangs like a Sword of Damocles over American manufacturers
## 5                                                                                                                                                                                                                           Trump has ultimately threatened tariffs on over     billion in Chinese goods covering virtually all US imports from China
## 6                                                                                                                                                                                                  The administration has imposed billions of dollars in tariffs on Chinese and Canadian products and threatened billions more on US allies in Europe
##   sentiment
## 1  Negative
## 2  Negative
## 3  Negative
## 4  Negative
## 5  Negative
## 6  Negative
table(tweet.final$sentiment)
## 
## Negative Positive 
##      700      700
tweet.final$id &lt;- seq(1, nrow(tweet.final))

tweet.text &lt;- tweet.final[,1]
#创建语料库
tweet.text &lt;- Corpus(VectorSource(tweet.text))
#去除停用词
tweet.text &lt;- tm_map(tweet.text,removeWords,stopwords())
## Warning in tm_map.SimpleCorpus(tweet.text, removeWords, stopwords()):
## transformation drops documents

tweet.dtm &lt;- DocumentTermMatrix(tweet.text)

freq.terms &lt;- sort(colSums(as.matrix(tweet.dtm)),decreasing = TRUE)
freq.terms &lt;- data.frame(name = names(freq.terms),
                         fre = freq.terms,row.names = NULL)
head(freq.terms,10)
##          name fre
## 1       china 931
## 2     chinese 659
## 3      threat 601
## 4       trump 331
## 5     tariffs 322
## 6       trade 290
## 7     threats 242
## 8  threatened 235
## 9     billion 193
## 10       said 188
length(which(freq.terms$fre&gt;100))
## [1] 27
library(wordcloud)
## Warning: 程辑包&#39;wordcloud&#39;是用R版本4.0.5 来建造的
## 载入需要的程辑包：RColorBrewer
## Warning: 程辑包&#39;RColorBrewer&#39;是用R版本4.0.5 来建造的
set.seed(623) # to make it reproducible
#词云图
wordcloud(words=freq.terms$name, freq=freq.terms$fre,
          min.freq = 50,random.color = FALSE,
          scale=c(4,0.5), colors=brewer.pal(8,&quot;Dark2&quot;))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
<pre class="r"><code>
## 将文本数据从语料库中转化为dataframe
polaritydf &lt;- data.frame(text=sapply(tweet.text, identity), stringsAsFactors=F)
polaritydf$label &lt;- rep(c(&quot;pos&quot;,&quot;neg&quot;),each = length(polaritydf$text)/2)
## 对比分析两种情感评论的用词长度
wordlist &lt;- str_split(polaritydf$text,&quot;[[:space:]]+&quot;)
wordlen &lt;- unlist(lapply(wordlist,length))
polaritydf$wordlen &lt;- wordlen

ggplot(polaritydf,aes(x = label,y = wordlen))+
  geom_violin(fill = &quot;red&quot;,alpha = 0.5)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-3.png" width="672" /></p>
<pre class="r"><code>#wilcox检验
wilcox.test(wordlen~label,data = polaritydf)
## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  wordlen by label
## W = 258990, p-value = 0.0642
## alternative hypothesis: true location shift is not equal to 0
mood.test(wordlen~label,data = polaritydf)
## 
##  Mood two-sample test of scale
## 
## data:  wordlen by label
## Z = -0.30545, p-value = 0.76
## alternative hypothesis: two.sided
##negtive word 的长度稍长
</code></pre>
<pre class="r"><code>wordfre &lt;- polaritydf%&gt;%unnest_tokens(word,text)%&gt;%
  group_by(label,word)%&gt;%
  summarise(Fre = n())%&gt;%
  arrange(desc(Fre)) %&gt;%
  acast(word~label,value.var = &quot;Fre&quot;,fill = 0)

## 可视化两种类型情感对比的词云
comparison.cloud(wordfre,scale=c(4,.5),max.words=180,
                 title.size=1.5,colors = c(&quot;darkgreen&quot;,&quot;darkred&quot;))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>#找出出现较为频繁的词语,出现频率大于5
dict &lt;- names(which(wordfre[,1]+wordfre[,2] &gt;5))
tw.dtm &lt;- DocumentTermMatrix(tweet.text,control = list(dictionary = dict))
#由于矩阵比较 稀疏,再控制一下稀疏程度
tw.dtm &lt;- removeSparseTerms(tw.dtm,0.99)
#随机抽取可视化tf-idf热力图
library(pheatmap)
index &lt;- sample(min(dim(tw.dtm)),100)
pheatmap(as.matrix(tw.dtm)[index,index],cluster_rows = FALSE,
         cluster_cols = FALSE,show_rownames = FALSE, 
         show_colnames = T,main = &quot;TF Matrix Part&quot;,
         fontsize_col = 5)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
<pre class="r"><code>
library(lda)
library(LDAvis)
library(servr)
term_table &lt;- table(unlist(wordlist))
term_table &lt;- sort(term_table, decreasing = TRUE)
term_table[1:20]
## 
##      China               Chinese     threat         US      Trump    tariffs 
##        928        721        659        595        532        330        316 
##      trade    threats threatened    billion       said        The      North 
##        249        240        235        192        185        179        167 
##          s      goods  President   security      Korea     Chinas 
##        164        163        156        138        136        125
dim(term_table)
## [1] 7210

## 剔除空的数据,否则json报错
del &lt;- (names(term_table)) %in% c(&quot;&quot;)
term_table &lt;- term_table[!del]
dim(term_table)
## [1] 7209

# 删除出现次数小于10次的词语
del &lt;- term_table &lt; 10
term_table &lt;- term_table[!del]
vocab &lt;- names(term_table)

# 将数据整理为lda包可使用的形式
get_terms &lt;- function(x) {
  index &lt;- match(x, vocab)
  index &lt;- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents &lt;- lapply(wordlist, get_terms)

## documents[[i]]：表示第i个文档
## documents[[i]][1,j] : 表示文档i中第j个单词对应在词库中的（索引－1）
## documents[[i]][2,j] : 表示文档i中第j个单词出现的次数

## 使用lda模型
alpha &lt;-  0.02;eta &lt;- 0.02

t1 &lt;- Sys.time()
fit &lt;- lda.collapsed.gibbs.sampler(documents = documents, K = 3, 
                                   vocab = vocab, 
                                   num.iterations = 50, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 &lt;- Sys.time()
t2 - t1  # about 30 sec on laptop
## Time difference of 0.1576049 secs
dim(fit$topics)
## [1]   3 536
## 可视化lda模型
## 对模型进行可视化

theta &lt;- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi &lt;- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
doc_length &lt;- sapply(documents, function(x) sum(x[2, ]))
term_frequency &lt;- as.vector(term_table)
pol_ldavis &lt;- list(phi = phi,theta = theta,doc_length = doc_length,
                   vocab = vocab,term_frequency = term_frequency)

# create the JSON object to feed the visualization:
json &lt;- createJSON(phi = pol_ldavis$phi, 
                   theta = pol_ldavis$theta, 
                   doc.length = pol_ldavis$doc_length, 
                   vocab = pol_ldavis$vocab, 
                   term.frequency = pol_ldavis$term_frequency)

serVis(json, out.dir = &quot;vis&quot;)</code></pre>
<pre class="r"><code>library(networkD3)
library(igraph)
library(RTextTools)
library(magrittr)
#bigram
TweetDf &lt;- data.frame(as.character(tweet.final$text))
names(TweetDf) &lt;- &quot;text&quot;
bi.gram.words &lt;- TweetDf %&gt;% 
  unnest_tokens(
    input = text, 
    output = bigram, 
    token = &#39;ngrams&#39;, 
    n = 2
  ) %&gt;% 
  filter(! is.na(bigram))
extra.stop.words &lt;- c(&#39;https&#39;)
stopwords.df &lt;- tibble(
  word = c(stopwords(kind = &#39;es&#39;),
           stopwords(kind = &#39;en&#39;),
           extra.stop.words)
)
bi.gram.words %&lt;&gt;% 
  separate(col = bigram, into = c(&#39;word1&#39;, &#39;word2&#39;), sep = &#39; &#39;) %&gt;% 
  filter(! word1 %in% stopwords.df$word) %&gt;% 
  filter(! word2 %in% stopwords.df$word) %&gt;% 
  filter(! is.na(word1)) %&gt;% 
  filter(! is.na(word2)) 
bi.gram.count &lt;- bi.gram.words %&gt;% 
  dplyr::count(word1, word2, sort = TRUE) %&gt;% 
  dplyr::rename(weight = n)


threshold &lt;- 50
filter=dplyr::filter
network &lt;-  bi.gram.count %&gt;%
  filter(weight &gt; threshold) %&gt;%
  graph_from_data_frame(directed = FALSE)

# Store the degree.
V(network)$degree &lt;- strength(graph = network)
# Compute the weight shares.
E(network)$width &lt;- E(network)$weight/max(E(network)$weight)

# Create networkD3 object.
network.D3 &lt;- igraph_to_networkD3(g = network)
# Define node size.
network.D3$nodes %&lt;&gt;% mutate(Degree = (1E-2)*V(network)$degree)
# Define color group
network.D3$nodes %&lt;&gt;% mutate(Group = 1)
# Define edges width. 
network.D3$links$Width &lt;- 10*E(network)$width

forceNetwork(
  Links = network.D3$links, 
  Nodes = network.D3$nodes, 
  Source = &#39;source&#39;, 
  Target = &#39;target&#39;,
  NodeID = &#39;name&#39;,
  Group = &#39;Group&#39;, 
  opacity = 0.9,
  Value = &#39;Width&#39;,
  Nodesize = &#39;Degree&#39;, 
  # We input a JavaScript function.
  linkWidth = JS(&quot;function(d) { return Math.sqrt(d.value); }&quot;), 
  fontSize = 12,
  zoom = TRUE, 
  opacityNoHover = 1
)</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="forceNetwork html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"links":{"source":[6,2,2,0,3,1,3,5,7],"target":[13,10,15,8,4,9,11,12,14],"value":[10,9.89795918367347,9.59183673469388,8.77551020408163,8.16326530612245,7.95918367346939,7.14285714285714,5.61224489795918,5.51020408163265],"colour":["#666","#666","#666","#666","#666","#666","#666","#666","#666"]},"nodes":{"name":["north","united","chinese","donald","president","trade","south","billion","korea","states","goods","trump","war","china","worth","imports"],"group":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"nodesize":[0.98,0.97,1.48,1.66,0.8,0.78,0.7,0.55,0.98,0.97,0.94,0.86,0.78,0.7,0.55,0.54]},"options":{"NodeID":"name","Group":"Group","colourScale":"d3.scaleOrdinal(d3.schemeCategory20);","fontSize":12,"fontFamily":"serif","clickTextSize":30,"linkDistance":50,"linkWidth":"function(d) { return Math.sqrt(d.value); }","charge":-30,"opacity":0.9,"zoom":true,"legend":false,"arrows":false,"nodesize":true,"radiusCalculation":" Math.sqrt(d.nodesize)+6","bounded":false,"opacityNoHover":1,"clickAction":null}},"evals":[],"jsHooks":[]}</script>
